{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samuelsp/projects/PML-DNN-hw1/neural_net\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import personalized_neural_net\n",
    "import neural_net\n",
    "print(os.getcwd())\n",
    "%pylab inline\n",
    "\n",
    "reload(personalized_neural_net)\n",
    "reload(neural_net)\n",
    "personalized_neural_net.reload_files()\n",
    "neural_net.reload_files()\n",
    "\n",
    "dropbox_path = '/Users/samuelsp/projects/PML-DNN-hw1/'\n",
    "filename = 'parkinsons_supervised.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping normalization for subject_num\n",
      "Original data length was 5875\n",
      "After dropping rows with nan in any label column, length is 5875\n",
      "3581 rows in training data\n",
      "1167 rows in validation data\n",
      "1127 rows in testing data\n",
      "\n",
      "Performing regression.\n",
      "Input dimensions (number of features): 20\n",
      "Number of classes/outputs: 2\n",
      "\n",
      "Building computation graph...\n",
      "last non-personalized layer size was\n",
      "32\n",
      "Okay, making a neural net with the following structure:\n",
      "Non-Personalized Layers:\n",
      "[('20x128', '128'), ('128x64', '64'), ('64x32', '32')]\n",
      "Personalized Layers:\n",
      "(42, 32, 8)\n",
      " and \n",
      "(42, 8, 2)\n",
      "hidden1 has shape\n",
      "(?, 128)\n",
      "hidden2 has shape\n",
      "(?, 64)\n",
      "hidden3 has shape\n",
      "(?, 32)\n",
      "returning output of hidden5!\n",
      "Training iteration 0\n",
      "\t Training RMSE 13.9145\n",
      "\t Validation RMSE 15.8712\n",
      "\t Loss 17.5762\n",
      "Training iteration 10000\n",
      "\t Training RMSE 7.68427\n",
      "\t Validation RMSE 7.27461\n",
      "\t Loss 7.99644\n",
      "Training iteration 20000\n",
      "\t Training RMSE 2.71063\n",
      "\t Validation RMSE 1.48773\n",
      "\t Loss 1.96575\n",
      "Training iteration 30000\n",
      "\t Training RMSE 1.609\n",
      "\t Validation RMSE 1.07993\n",
      "\t Loss 1.44677\n",
      "Training iteration 40000\n",
      "\t Training RMSE 1.68694\n",
      "\t Validation RMSE 1.18957\n",
      "\t Loss 1.52954\n",
      "WARNING! Only test on the test set when you have finished choosing all of your hyperparameters!\n",
      "\tNever use the test set to choose hyperparameters!!!\n",
      "Final RMSE on test data is: 1.13606\n",
      "logits_metrics are\n",
      "42\n",
      "[[30.285753, 38.364914, 1.0503435, 1.7294444], [13.857381, 16.321493, 2.4775307, 2.5674424], [29.254274, 34.14975, 2.47825, 4.5847769], [14.713261, 22.169645, 1.2651252, 2.1522353], [33.290318, 43.860157, 0.96528751, 1.5381304], [27.115984, 40.555805, 0.50471908, 1.3869632], [16.267542, 23.338118, 0.87763929, 1.2122463], [20.32464, 26.281328, 1.5699772, 1.557102], [18.002689, 24.783848, 0.66128224, 0.43781999], [13.213478, 19.777201, 1.5621908, 1.0187376], [18.696836, 22.454, 1.3683103, 0.90926868], [16.807253, 24.137218, 1.1078905, 1.3879246], [20.645424, 28.961226, 2.4222922, 2.4216902], [14.654179, 19.785643, 3.44277, 3.8866875], [13.978317, 19.888456, 0.73279196, 1.384445], [8.6441364, 18.095299, 0.2089517, 1.9483297], [24.947021, 30.422886, 1.32701, 1.4090029], [6.0009513, 7.3728013, 0.028882647, 0.037784584], [17.07864, 25.705776, 0.15283383, 0.21730851], [11.142343, 16.854116, 0.2608681, 0.28454399], [32.160221, 43.233582, 3.5080535, 3.7248733], [10.037972, 11.175005, 1.3537234, 1.9919223], [13.34188, 25.117041, 0.36679235, 0.97040468], [13.090498, 17.448177, 0.12901203, 0.36422604], [32.252434, 51.300083, 2.8621452, 3.4713752], [25.553915, 32.141087, 1.2254912, 1.9271241], [10.69115, 15.373566, 0.55627745, 0.93665391], [28.776033, 34.859768, 2.2918227, 4.4594936], [24.326918, 32.709888, 0.35565782, 1.1460196], [25.279985, 35.107731, 2.9073648, 2.92243], [25.714598, 28.935423, 2.700598, 2.2700279], [9.9561348, 13.079796, 1.9927601, 1.4741496], [27.026831, 30.867476, 2.1127205, 1.5737119], [24.267586, 31.930254, 0.60924429, 0.47488505], [34.468937, 52.504417, 0.13063155, 0.35542929], [23.085011, 29.728987, 0.84492433, 0.95342213], [33.122875, 45.062576, 2.5087321, 3.4268069], [19.775234, 27.048735, 0.25653893, 0.64833045], [31.097189, 41.381153, 2.1945331, 2.5589445], [15.896788, 26.552366, 0.17279175, 1.3686876], [32.234985, 40.440735, 0.63882965, 0.80709237], [23.725151, 34.303886, 1.2434472, 1.6192149]]\n"
     ]
    }
   ],
   "source": [
    "#train and test the personalized NN, with best known Hyperparamters\n",
    "\n",
    "layer_sizes = [128,64,32]\n",
    "learning_rate = 0.01\n",
    "weight_penalty = 0.01\n",
    "dropout_prob = 0.9\n",
    "batch_size = 25\n",
    "clip_gradients = True\n",
    "results_dict = {}\n",
    "\n",
    "for i in range(0, 1):\n",
    "    net = personalized_neural_net.NeuralNetwork(dropbox_path + filename, 'homework', \n",
    "                               layer_sizes, batch_size, learning_rate, dropout_prob, \n",
    "                               weight_penalty, clip_gradients, model_type='regression')\n",
    "\n",
    "    net.train(num_steps=50000, output_every_nth=10000)\n",
    "    p_scores, p_pop_logits = net.test_on_test_with_logits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6604311, 1.47884, 1.0939726, 2.276233, 3.3208604, 1.2490082, 1.5360519, 0.48253295, 0.98704135, 2.264416, 2.2558765, 1.4302554, 1.5232713, 1.0706556, 1.7809583, 2.4024317, 1.3883036, 2.5200899, 1.444953, 1.0774845, 2.3162267, 2.3820944, 2.5746593, 1.7725923, 1.8189143, 1.5414107, 0.88816482, 1.5169003, 2.5889919, 1.2312341, 1.5888368, 1.3951801, 1.2890769, 1.9704896, 2.7139347, 2.3648372, 2.2799838, 4.4449182, 3.5257034, 1.0933925, 1.8917871, 1.9345043]\n",
      "[33.980267, 44.094105, 2.6180506, 3.8467669]\n",
      "[29.238634, 39.077618, 2.3349311, 2.9040568]\n",
      "[16.113918, 23.255951, 0.945279, 1.3509676]\n",
      "[15.492904, 21.22257, 0.82235205, 1.1540991]\n",
      "[14.171157, 20.488329, 2.2154064, 2.8131752]\n",
      "[6.6446304, 10.197608, 0.91327745, 1.4781189]\n",
      "[30.484179, 47.915298, 3.3903756, 3.5806532]\n",
      "[25.295914, 33.223763, 1.1728668, 2.5620675]\n",
      "[37.77528, 53.774567, 1.7457308, 3.2302155]\n",
      "[17.100428, 25.615669, 0.55666339, 0.43002868]\n"
     ]
    }
   ],
   "source": [
    "print p_scores\n",
    "\n",
    "print p_pop_logits[0]\n",
    "print p_pop_logits[4]\n",
    "print p_pop_logits[6]\n",
    "print p_pop_logits[9]\n",
    "print p_pop_logits[14]\n",
    "print p_pop_logits[17]\n",
    "print p_pop_logits[24]\n",
    "print p_pop_logits[28]\n",
    "print p_pop_logits[34]\n",
    "print p_pop_logits[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping normalization for subject_num\n",
      "Original data length was 5875\n",
      "After dropping rows with nan in any label column, length is 5875\n",
      "3581 rows in training data\n",
      "1167 rows in validation data\n",
      "1127 rows in testing data\n",
      "\n",
      "Performing regression.\n",
      "Input dimensions (number of features): 20\n",
      "Number of classes/outputs: 2\n",
      "\n",
      "Building computation graph...\n",
      "Okay, making a neural net with the following structure:\n",
      "[('20x128', '128'), ('128x64', '64'), ('64x32', '32'), ('32x2', '2')]\n",
      "Training iteration 0\n",
      "\t Training RMSE 27.8194\n",
      "\t Validation RMSE 27.4898\n",
      "\t Loss 28.3684\n",
      "Training iteration 1000\n",
      "\t Training RMSE 5.64127\n",
      "\t Validation RMSE 7.45095\n",
      "\t Loss 7.96597\n",
      "Training iteration 2000\n",
      "\t Training RMSE 6.54474\n",
      "\t Validation RMSE 6.83688\n",
      "\t Loss 7.32517\n",
      "Training iteration 3000\n",
      "\t Training RMSE 6.8932\n",
      "\t Validation RMSE 6.01702\n",
      "\t Loss 6.51097\n",
      "Training iteration 4000\n",
      "\t Training RMSE 5.34253\n",
      "\t Validation RMSE 5.34437\n",
      "\t Loss 5.85284\n",
      "Training iteration 5000\n",
      "\t Training RMSE 6.57459\n",
      "\t Validation RMSE 4.90101\n",
      "\t Loss 5.41362\n",
      "Training iteration 6000\n",
      "\t Training RMSE 5.85033\n",
      "\t Validation RMSE 4.73483\n",
      "\t Loss 5.25485\n",
      "Training iteration 7000\n",
      "\t Training RMSE 4.55385\n",
      "\t Validation RMSE 4.28636\n",
      "\t Loss 4.82802\n",
      "Training iteration 8000\n",
      "\t Training RMSE 5.86356\n",
      "\t Validation RMSE 4.29927\n",
      "\t Loss 4.84963\n",
      "Training iteration 9000\n",
      "\t Training RMSE 4.19564\n",
      "\t Validation RMSE 4.06237\n",
      "\t Loss 4.61515\n",
      "Training iteration 10000\n",
      "\t Training RMSE 4.80826\n",
      "\t Validation RMSE 3.82488\n",
      "\t Loss 4.378\n",
      "Training iteration 11000\n",
      "\t Training RMSE 3.69878\n",
      "\t Validation RMSE 4.17947\n",
      "\t Loss 4.74182\n",
      "Training iteration 12000\n",
      "\t Training RMSE 3.86282\n",
      "\t Validation RMSE 3.88435\n",
      "\t Loss 4.45383\n",
      "Training iteration 13000\n",
      "\t Training RMSE 3.57444\n",
      "\t Validation RMSE 3.62269\n",
      "\t Loss 4.19528\n",
      "Training iteration 14000\n",
      "\t Training RMSE 4.97085\n",
      "\t Validation RMSE 3.51922\n",
      "\t Loss 4.1067\n",
      "Training iteration 15000\n",
      "\t Training RMSE 3.81441\n",
      "\t Validation RMSE 3.50516\n",
      "\t Loss 4.09081\n",
      "Training iteration 16000\n",
      "\t Training RMSE 4.14331\n",
      "\t Validation RMSE 3.48477\n",
      "\t Loss 4.07214\n",
      "Training iteration 17000\n",
      "\t Training RMSE 2.91505\n",
      "\t Validation RMSE 3.51686\n",
      "\t Loss 4.11667\n",
      "Training iteration 18000\n",
      "\t Training RMSE 3.05299\n",
      "\t Validation RMSE 3.26223\n",
      "\t Loss 3.86889\n",
      "Training iteration 19000\n",
      "\t Training RMSE 3.48721\n",
      "\t Validation RMSE 3.26055\n",
      "\t Loss 3.87485\n",
      "Training iteration 20000\n",
      "\t Training RMSE 3.10812\n",
      "\t Validation RMSE 3.5333\n",
      "\t Loss 4.14882\n",
      "Training iteration 21000\n",
      "\t Training RMSE 3.99797\n",
      "\t Validation RMSE 3.04966\n",
      "\t Loss 3.67355\n",
      "Training iteration 22000\n",
      "\t Training RMSE 3.06102\n",
      "\t Validation RMSE 3.13959\n",
      "\t Loss 3.76575\n",
      "Training iteration 23000\n",
      "\t Training RMSE 4.0855\n",
      "\t Validation RMSE 3.58101\n",
      "\t Loss 4.21087\n",
      "Training iteration 24000\n",
      "\t Training RMSE 4.08929\n",
      "\t Validation RMSE 3.11217\n",
      "\t Loss 3.76012\n",
      "Training iteration 25000\n",
      "\t Training RMSE 3.30931\n",
      "\t Validation RMSE 3.0802\n",
      "\t Loss 3.73878\n",
      "Training iteration 26000\n",
      "\t Training RMSE 3.67662\n",
      "\t Validation RMSE 3.06294\n",
      "\t Loss 3.71261\n",
      "Training iteration 27000\n",
      "\t Training RMSE 4.56935\n",
      "\t Validation RMSE 2.98918\n",
      "\t Loss 3.65675\n",
      "Training iteration 28000\n",
      "\t Training RMSE 2.39403\n",
      "\t Validation RMSE 2.80145\n",
      "\t Loss 3.47649\n",
      "Training iteration 29000\n",
      "\t Training RMSE 3.04712\n",
      "\t Validation RMSE 2.86709\n",
      "\t Loss 3.54938\n",
      "Training iteration 30000\n",
      "\t Training RMSE 3.37113\n",
      "\t Validation RMSE 2.77251\n",
      "\t Loss 3.46421\n",
      "Training iteration 31000\n",
      "\t Training RMSE 3.06417\n",
      "\t Validation RMSE 2.85748\n",
      "\t Loss 3.54935\n",
      "Training iteration 32000\n",
      "\t Training RMSE 3.4612\n",
      "\t Validation RMSE 2.75738\n",
      "\t Loss 3.45692\n",
      "Training iteration 33000\n",
      "\t Training RMSE 3.41462\n",
      "\t Validation RMSE 2.76097\n",
      "\t Loss 3.46964\n",
      "Training iteration 34000\n",
      "\t Training RMSE 2.91602\n",
      "\t Validation RMSE 2.61936\n",
      "\t Loss 3.33554\n",
      "Training iteration 35000\n",
      "\t Training RMSE 3.6499\n",
      "\t Validation RMSE 2.58201\n",
      "\t Loss 3.30336\n",
      "Training iteration 36000\n",
      "\t Training RMSE 4.47943\n",
      "\t Validation RMSE 2.65229\n",
      "\t Loss 3.37215\n",
      "Training iteration 37000\n",
      "\t Training RMSE 2.56712\n",
      "\t Validation RMSE 2.71498\n",
      "\t Loss 3.4351\n",
      "Training iteration 38000\n",
      "\t Training RMSE 3.57696\n",
      "\t Validation RMSE 2.47115\n",
      "\t Loss 3.20207\n",
      "Training iteration 39000\n",
      "\t Training RMSE 2.37367\n",
      "\t Validation RMSE 2.89786\n",
      "\t Loss 3.63088\n",
      "Training iteration 40000\n",
      "\t Training RMSE 3.04215\n",
      "\t Validation RMSE 2.71295\n",
      "\t Loss 3.44742\n",
      "Training iteration 41000\n",
      "\t Training RMSE 4.19565\n",
      "\t Validation RMSE 2.49412\n",
      "\t Loss 3.23418\n",
      "Training iteration 42000\n",
      "\t Training RMSE 3.02531\n",
      "\t Validation RMSE 2.37655\n",
      "\t Loss 3.11175\n",
      "Training iteration 43000\n",
      "\t Training RMSE 3.55596\n",
      "\t Validation RMSE 2.31442\n",
      "\t Loss 3.06\n",
      "Training iteration 44000\n",
      "\t Training RMSE 2.74456\n",
      "\t Validation RMSE 2.5659\n",
      "\t Loss 3.31129\n",
      "Training iteration 45000\n",
      "\t Training RMSE 2.92392\n",
      "\t Validation RMSE 2.46505\n",
      "\t Loss 3.21121\n",
      "Training iteration 46000\n",
      "\t Training RMSE 3.49185\n",
      "\t Validation RMSE 2.36282\n",
      "\t Loss 3.11119\n",
      "Training iteration 47000\n",
      "\t Training RMSE 3.14183\n",
      "\t Validation RMSE 2.31275\n",
      "\t Loss 3.05775\n",
      "Training iteration 48000\n",
      "\t Training RMSE 3.40384\n",
      "\t Validation RMSE 2.60361\n",
      "\t Loss 3.35191\n",
      "Training iteration 49000\n",
      "\t Training RMSE 2.73201\n",
      "\t Validation RMSE 2.58866\n",
      "\t Loss 3.33911\n",
      "WARNING! Only test on the test set when you have finished choosing all of your hyperparameters!\n",
      "\tNever use the test set to choose hyperparameters!!!\n",
      "Final RMSE on test data is: 2.20947\n",
      "logits_metrics are\n",
      "42\n",
      "[[32.724976, 42.058895, 2.3727791, 3.4611075], [12.990462, 16.058235, 2.2984345, 3.2614822], [24.537243, 30.504618, 2.1072538, 3.6239142], [17.045744, 24.105385, 2.3910446, 3.3124847], [30.797865, 41.03323, 2.1512988, 2.8076642], [25.901197, 38.059708, 1.3709685, 2.5824871], [16.439047, 23.778788, 0.88084573, 1.0286], [19.252388, 25.370522, 1.3302927, 1.7323388], [17.472525, 24.428886, 1.4859027, 1.7982546], [14.75786, 20.439133, 0.95268089, 1.6333122], [16.62599, 21.548716, 1.0840349, 1.1266245], [16.324301, 23.276997, 1.2480409, 1.8297373], [21.143448, 27.748659, 2.1100314, 2.1620345], [13.4619, 18.91943, 2.8559113, 4.4111977], [13.182796, 19.068289, 2.3404493, 3.341325], [10.619169, 16.925648, 0.83526075, 1.3909656], [23.847855, 31.024591, 3.2843585, 4.3192677], [7.411324, 10.864024, 1.0925162, 2.0109518], [17.862164, 25.331781, 0.80949283, 0.99880117], [11.09398, 16.307644, 1.0874437, 1.2846738], [27.148987, 37.527969, 2.6372294, 4.6892977], [9.2623186, 12.650997, 1.3080904, 2.3167243], [14.953821, 23.306814, 1.3858421, 2.1947205], [13.859275, 19.15801, 1.0318035, 1.549009], [30.660845, 47.410442, 2.9869523, 3.4841065], [24.147715, 32.362938, 1.2819175, 1.6583613], [10.751038, 15.58914, 0.78915, 1.8416494], [27.25515, 33.957661, 2.7477341, 4.0661602], [25.30526, 32.748978, 0.74664891, 1.275793], [26.24474, 35.851109, 3.63643, 4.4274745], [25.015253, 29.9732, 2.4160471, 2.6009924], [10.023354, 12.899108, 1.9728235, 2.1538837], [25.612881, 31.034687, 0.98416203, 1.0790786], [23.538971, 32.222973, 0.81035131, 1.0470648], [35.188251, 50.265068, 2.0744584, 3.5489917], [23.362709, 29.937042, 1.8377672, 1.6470788], [31.164436, 40.089451, 3.2598166, 4.5142717], [22.154018, 30.420078, 1.9482523, 2.5283597], [27.370287, 37.231453, 2.5909896, 3.4982338], [16.884161, 24.997168, 0.66331273, 0.67736804], [33.13205, 41.267914, 2.0708365, 2.4833281], [24.264574, 33.739548, 0.503443, 0.94913399]]\n"
     ]
    }
   ],
   "source": [
    "# train and test the baseline NN, with best known Hyperparamters\n",
    "\n",
    "layer_sizes = [128,64,32]\n",
    "learning_rate = 0.001\n",
    "weight_penalty = 0.01\n",
    "dropout_prob = 0.9\n",
    "batch_size = 25\n",
    "clip_gradients = True\n",
    "results_dict = {}\n",
    "\n",
    "for i in range(0, 1):\n",
    "    net = neural_net.NeuralNetwork(dropbox_path + filename, 'homework', \n",
    "                               layer_sizes, batch_size, learning_rate, dropout_prob, \n",
    "                               weight_penalty, clip_gradients, model_type='regression')\n",
    "\n",
    "    net.train(num_steps=50000, output_every_nth=1000)    \n",
    "    scores, pop_logits = net.test_on_test_with_logits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.90398\n",
      "1.90682\n",
      "0.755528\n",
      "0.29862\n",
      "1.75986\n",
      "0.351995\n",
      "3.08455\n",
      "1.45458\n",
      "1.96584\n",
      "2.4768\n",
      "[30.285753, 38.364914, 1.0503435, 1.7294444]\n",
      "[33.290318, 43.860157, 0.96528751, 1.5381304]\n",
      "[16.267542, 23.338118, 0.87763929, 1.2122463]\n",
      "[13.213478, 19.777201, 1.5621908, 1.0187376]\n",
      "[13.978317, 19.888456, 0.73279196, 1.384445]\n",
      "[6.0009513, 7.3728013, 0.028882647, 0.037784584]\n",
      "[32.252434, 51.300083, 2.8621452, 3.4713752]\n",
      "[24.326918, 32.709888, 0.35565782, 1.1460196]\n",
      "[34.468937, 52.504417, 0.13063155, 0.35542929]\n",
      "[15.896788, 26.552366, 0.17279175, 1.3686876]\n",
      "30.285$\\pm$1.050 & 38.364$\\pm$1.729\n",
      "\n",
      "33.290$\\pm$0.965 & 43.860$\\pm$1.538\n",
      "\n",
      "16.267$\\pm$0.877 & 23.338$\\pm$1.212\n",
      "\n",
      "13.213$\\pm$1.562 & 19.777$\\pm$1.018\n",
      "\n",
      "13.978$\\pm$0.732 & 19.888$\\pm$1.384\n",
      "\n",
      "6.000$\\pm$0.028 & 7.372$\\pm$0.037\n",
      "\n",
      "32.252$\\pm$2.862 & 51.300$\\pm$3.471\n",
      "\n",
      "24.326$\\pm$0.355 & 32.709$\\pm$1.146\n",
      "\n",
      "34.468$\\pm$0.130 & 52.504$\\pm$0.355\n",
      "\n",
      "15.896$\\pm$0.172 & 26.552$\\pm$1.368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print p_scores[0]\n",
    "print p_scores[4]\n",
    "print p_scores[6]\n",
    "print p_scores[9]\n",
    "print p_scores[14]\n",
    "print p_scores[17]\n",
    "print p_scores[24]\n",
    "print p_scores[28]\n",
    "print p_scores[34]\n",
    "print p_scores[29]\n",
    "\n",
    "print p_pop_logits[0]\n",
    "print p_pop_logits[4]\n",
    "print p_pop_logits[6]\n",
    "print p_pop_logits[9]\n",
    "print p_pop_logits[14]\n",
    "print p_pop_logits[17]\n",
    "print p_pop_logits[24]\n",
    "print p_pop_logits[28]\n",
    "print p_pop_logits[34]\n",
    "print p_pop_logits[39]\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25564\n",
      "2.37159\n",
      "1.7492\n",
      "1.64089\n",
      "1.87341\n",
      "3.27142\n",
      "2.40171\n",
      "1.82284\n",
      "4.18068\n",
      "1.74175\n",
      "[32.724976, 42.058895, 2.3727791, 3.4611075]\n",
      "[30.797865, 41.03323, 2.1512988, 2.8076642]\n",
      "[16.439047, 23.778788, 0.88084573, 1.0286]\n",
      "[14.75786, 20.439133, 0.95268089, 1.6333122]\n",
      "[13.182796, 19.068289, 2.3404493, 3.341325]\n",
      "[7.411324, 10.864024, 1.0925162, 2.0109518]\n",
      "[30.660845, 47.410442, 2.9869523, 3.4841065]\n",
      "[25.30526, 32.748978, 0.74664891, 1.275793]\n",
      "[35.188251, 50.265068, 2.0744584, 3.5489917]\n",
      "[16.884161, 24.997168, 0.66331273, 0.67736804]\n"
     ]
    }
   ],
   "source": [
    "print scores[0]\n",
    "print scores[4]\n",
    "print scores[6]\n",
    "print scores[9]\n",
    "print scores[14]\n",
    "print scores[17]\n",
    "print scores[24]\n",
    "print scores[28]\n",
    "print scores[34]\n",
    "print scores[29]\n",
    "\n",
    "print pop_logits[0]\n",
    "print pop_logits[4]\n",
    "print pop_logits[6]\n",
    "print pop_logits[9]\n",
    "print pop_logits[14]\n",
    "print pop_logits[17]\n",
    "print pop_logits[24]\n",
    "print pop_logits[28]\n",
    "print pop_logits[34]\n",
    "print pop_logits[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|c|c|c|c|c|}\n",
      "\\hline\n",
      "Subject \\# & Test RMSE & Mean Test label\\_motor\\_UPDRS & Mean Test label\\_total\\_UPDRS\\\\\n",
      "\\hline\n",
      "1 & 1.255 & 32.724$\\pm$2.372 & 42.058$\\pm$3.461\\\\\n",
      "\n",
      "5 & 2.371 & 30.797$\\pm$2.151 & 41.033$\\pm$2.807\\\\\n",
      "\n",
      "7 & 1.749 & 16.439$\\pm$0.880 & 23.778$\\pm$1.028\\\\\n",
      "\n",
      "10 & 1.640 & 14.757$\\pm$0.952 & 20.439$\\pm$1.633\\\\\n",
      "\n",
      "15 & 1.873 & 13.182$\\pm$2.340 & 19.068$\\pm$3.341\\\\\n",
      "\n",
      "18 & 3.271 & 7.411$\\pm$1.092 & 10.864$\\pm$2.010\\\\\n",
      "\n",
      "25 & 2.401 & 30.660$\\pm$2.986 & 47.410$\\pm$3.484\\\\\n",
      "\n",
      "29 & 1.822 & 25.305$\\pm$0.746 & 32.748$\\pm$1.275\\\\\n",
      "\n",
      "35 & 4.180 & 35.188$\\pm$2.074 & 50.265$\\pm$3.548\\\\\n",
      "\n",
      "40 & 1.334 & 16.884$\\pm$0.663 & 24.997$\\pm$0.677\\\\\n",
      "\n",
      "\\hline\n",
      "\\end{tabular} \\\\\n",
      "\\newline\n",
      "(1.8025361335934964, 0.16248230441343942)\n",
      "1.53033032417\n",
      "(2.3436985059168531, 0.22995124595541086)\n",
      "2.19016088247\n"
     ]
    }
   ],
   "source": [
    "n_test_points = [0,89,0,0,0,96,0,97,0,0,82,0,0,0,0,91,0,0,74,0,0,0,0,0,0,89,0,0,0,93,0,0,0,0,0,94,0,0,0,0,95]\n",
    "\n",
    "def get_overall_rmse(data):\n",
    "    sum1 = 0\n",
    "    mae = 0\n",
    "    total_points = 0\n",
    "    for i in [1, 5, 7, 10, 15, 18, 25, 29, 35, 40]:\n",
    "        sum1 = sum1 + (data[i-1]**2)*n_test_points[i]\n",
    "        mae = mae + abs(((data[i-1]**2)*n_test_points[i]))**0.5\n",
    "        total_points = total_points + n_test_points[i]\n",
    "        \n",
    "    sum1 = sum1 / total_points\n",
    "    mae = mae/total_points\n",
    "    sum1 = sum1 ** 0.5\n",
    "    return sum1, mae \n",
    "\n",
    "def get_mean_rmse(input_scores):\n",
    "    mean_rmse = 0\n",
    "    for i in [1, 5, 7, 10, 15, 18, 25, 29, 35, 40]:\n",
    "        mean_rmse += input_scores[i-1]\n",
    "    return(mean_rmse/10)\n",
    "\n",
    "def truncate(f, n):\n",
    "    '''Truncates/pads a float f to n decimal places without rounding'''\n",
    "    s = '{}'.format(f)\n",
    "    if 'e' in s or 'E' in s:\n",
    "        return '{0:.{1}f}'.format(f, n)\n",
    "    i, p, d = s.partition('.')\n",
    "    return '.'.join([i, (d+'0'*n)[:n]])\n",
    "\n",
    "def gen_latex_table(input_logits, input_scores):\n",
    "    \n",
    "    print \"\\\\begin{tabular}{|c|c|c|c|c|}\"\n",
    "    print \"\\\\hline\"\n",
    "    print \"Subject \\\\# & Test RMSE & Mean Test label\\\\_motor\\\\_UPDRS & Mean Test label\\\\_total\\\\_UPDRS\\\\\\\\\"\n",
    "    print \"\\\\hline\"\n",
    "    for i in [1, 5, 7, 10, 15, 18, 25, 29, 35, 40]:\n",
    "        s = str(i) + ' & ' + str(truncate(input_scores[i-1],3)) + ' & '\n",
    "        print s + str(truncate(input_logits[i-1][0], 3)) + '$\\pm$' + str(truncate(input_logits[i-1][2], 3)) + ' & ' + str(truncate(input_logits[i-1][1], 3)) + '$\\pm$' + str(truncate(input_logits[i-1][3], 3)) + \"\\\\\\\\\" + '\\n'\n",
    "        \n",
    "    print \"\\\\hline\"\n",
    "    print \"\\\\end{tabular} \\\\\\\\\"\n",
    "    print \"\\\\newline\"\n",
    "        \n",
    "#gen_latex_table(p_pop_logits, p_scores) \n",
    "gen_latex_table(pop_logits, scores)\n",
    "\n",
    "\n",
    "print get_overall_rmse(p_scores)\n",
    "print get_mean_rmse(p_scores)\n",
    "print get_overall_rmse(scores)\n",
    "print get_mean_rmse(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
